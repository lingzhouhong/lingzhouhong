---
abstract: This paper considers a learning problem with heteroscedastic and correlated data that is distributed across nodes. We propose a distributed learning scheme where each node asynchronously implements stochastic gradient descent updates and exchanges their current models with neighbors. We ensure the similarity among the local models and the ensemble average by having a network regularization penalty to the least squares problem.  This penalty is associated with weights that are proportional to the relative accuracy of local models. We provide finite time characterization of the disparity between local models and the ensemble average model based on the penalty constants and network connectivity. We compare the proposed method with generalized least squares and logistic regression in the prediction of activities of individuals based on head movement data.
tags:
authors:
- admin
- Alfredo Garcia
- Ceyhun Eksin
date: "2020-12-14T00:00:00Z"
doi: ""
featured: true
image:
  caption: 'CDC IEEE 2020'
  focal_point: ""
  preview_only: false
links:
- name: NSF Link
  url: https://par.nsf.gov/biblio/10233511/
- name: slides
  url: uploads/IEEE_Conference.pdf
projects:
- Optimization Research
publication: In *IEEE Conference on Decision and Control (CDC)*
publication_short: In *IEEE CDC*
publication_types:
- "1"
publishDate: "2017-01-01T00:00:00Z"
summary: ""
tags:
- Optimization
- Distributed Networked Learning
- IEEE CDC
title: Distributed Networked Learning with Correlated Data
---

